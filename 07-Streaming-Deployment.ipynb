{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Streaming Deployment\n\nAfter batch deployment, continuous model inference using a technology like Spark Streaming represents the second most popular deployment option.  This lesson introduces Spark Streaming and how to perform inference on a stream of incoming data.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Make predictions on streaming data\n - Connect to a Spark Stream\n - Predict using an `sklearn` model on a stream of data\n - Stream predictions into an always up-to-date parquet file"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Inference on Streaming Data\n\nSpark Streaming enables...<br><br>\n\n* Scalable and fault-tolerant operations that continuously perform inference on incoming data\n* Streaming applications can also incorporate ETL and other Spark features to trigger actions in real time\n\nThis lesson is meant as an introduction to streaming applications as they pertain to production machine learning jobs.  \n\nStreaming poses a number of specific obstacles. These obstacles include:<br><br>\n\n* *End-to-end reliability and correctness:* Applications must be resilient to failures of any element of the pipeline caused by network issues, traffic spikes, and/or hardware malfunctions\n* *Handle complex transformations:* applications receive many data formats that often involve complex business logic\n* *Late and out-of-order data:* network issues can result in data that arrives late and out of its intended order\n* *Integrate with other systems:* Applications must integrate with the rest of a data infrastructure"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nStreaming data sources in Spark...<br><br>\n\n* Offer the same DataFrames API for interacting with your data\n* The crucial difference is that in structured streaming, the DataFrame is unbounded\n* In other words, data arrives in an input stream and new records are appended to the input DataFrame\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/structured-streamining-model.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nSpark is a good solution for...<br><br>\n\n* Batch inference\n* Incoming streams of data\n\nFor low-latency inference, however, Spark may or may not be the best solution depending on the latency demands of your task"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["-sandbox\n### Connecting to the Stream\n\nAs data technology matures, the industry has been converging on a set of technologies.  Apache Kafka and the Azure managed alternative Event Hubs have become the ingestion engine at the heart of many pipelines.  \n\nThis technology brokers messages between producers, such as an IoT device writing data, and consumers, such as a Spark cluster reading data to perform real time analytics. There can be a many-to-many relationship between producers and consumers and the broker itself is scalable and fault tolerant.\n\nWe'll simulate a stream using the `maxFilesPerTrigger` option.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/>  There are a number of ways to stream data.  One other common design pattern is to stream from an Azure Blob Container where any new files that appear will be read by the stream."],"metadata":{}},{"cell_type":"markdown","source":["Import the dataset in Spark."],"metadata":{}},{"cell_type":"code","source":["airbnbDF = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet/\")\n\ndisplay(airbnbDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Create a schema for the data stream."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType, IntegerType, StructType\n\nschema = (StructType()\n.add(\"host_total_listings_count\", DoubleType())\n.add(\"neighbourhood_cleansed\", IntegerType())\n.add(\"zipcode\", IntegerType())\n.add(\"latitude\", DoubleType())\n.add(\"longitude\", DoubleType())\n.add(\"property_type\", IntegerType())\n.add(\"room_type\", IntegerType())\n.add(\"accommodates\", DoubleType())\n.add(\"bathrooms\", DoubleType())\n.add(\"bedrooms\", DoubleType())\n.add(\"beds\", DoubleType())\n.add(\"bed_type\", IntegerType())\n.add(\"minimum_nights\", DoubleType())\n.add(\"number_of_reviews\", DoubleType())\n.add(\"review_scores_rating\", DoubleType())\n.add(\"review_scores_accuracy\", DoubleType())\n.add(\"review_scores_cleanliness\", DoubleType())\n.add(\"review_scores_checkin\", DoubleType())\n.add(\"review_scores_communication\", DoubleType())\n.add(\"review_scores_location\", DoubleType())\n.add(\"review_scores_value\", DoubleType())\n.add(\"price\", DoubleType())\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Check to make sure the schemas match."],"metadata":{}},{"cell_type":"code","source":["schema == airbnbDF.schema"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Check the number of shuffle partitions."],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Change this to 8."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Create a data stream using `readStream` and `maxFilesPerTrigger`."],"metadata":{}},{"cell_type":"code","source":["streamingData = (spark\n                 .readStream\n                 .schema(schema)\n                 .option(\"maxFilesPerTrigger\", 1)\n                 .parquet(\"/mnt/conor-work/airbnb/airbnb-cleaned-mlflow.parquet\")\n                 .drop(\"price\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Apply an `sklearn` Model on the Stream\n\nUsing the DataFrame API, Spark allows us to interact with a stream of incoming data in much the same way that we did with a batch of data."],"metadata":{}},{"cell_type":"markdown","source":["Import a `spark_udf`"],"metadata":{}},{"cell_type":"code","source":["import mlflow\nimport mlflow.sklearn\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\nwith mlflow.start_run(run_name=\"Final RF Model\") as run: \n  df = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")\n  X = df.drop([\"price\"], axis=1)\n  y = df[\"price\"]\n\n  rf = RandomForestRegressor(n_estimators=100, max_depth=5)\n  rf.fit(X, y)\n  \n  mlflow.sklearn.log_model(rf, \"random-forest-model\")\n  \n  runID = run.info.run_uuid\n  experimentId = run.info.experiment_id"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["import mlflow.pyfunc\n\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, \"random-forest-model\", run_id=runID)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Transform the stream with a prediction."],"metadata":{}},{"cell_type":"code","source":["predictionsDF = streamingData.withColumn(\"prediction\", pyfunc_udf(*streamingData.columns))\n\ndisplay(predictionsDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Write out a Stream of Predictions\n\nYou can perform writes to any target database.  In this case, write to a parquet file.  This file will always be up to date, another component of an application can query this endpoint at any time."],"metadata":{}},{"cell_type":"code","source":["checkpointLocation = userhome + \"/academy/stream.checkpoint\"\nwritePath = userhome + \"/academy/predictions\"\n\n(predictionsDF\n  .writeStream                                           # Write the stream\n  .format(\"delta\")                                       # Use the delta format\n  .partitionBy(\"zipcode\")                                # Specify a feature to partition on\n  .option(\"checkpointLocation\", checkpointLocation)      # Specify where to log metadata\n  .option(\"path\", writePath)                             # Specify the output path\n  .outputMode(\"append\")                                  # Append new records to the output path\n  .start()                                               # Start the operation\n)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Take a look at the underlying file.  Refresh this a few times."],"metadata":{}},{"cell_type":"code","source":["spark.read.format(\"delta\").load(writePath).count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Stop the stream."],"metadata":{}},{"cell_type":"code","source":["[q.stop() for q in spark.streams.active]"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Things to note:<br><br>\n\n* For batch processing, you can trigger a stream every 24 hours to maintain state\n* You can easily combine historic and new data in the same stream"],"metadata":{}},{"cell_type":"markdown","source":["## Review\n\n**Question:** What are commonly approached as data streams?  \n**Answer:** Apache Kafka and the Azure managed alternative Event Hubs are common data streams.  Additionally, it's common to monitor a directory for incoming files.  When a new file appears, it is brought into the stream for processing.\n\n**Question:** How does Spark ensure exactly-once data delivery and maintain metadata on a stream?  \n**Answer:** Checkpoints give Spark this fault tolerance through the ability to maintain state off of the cluster.\n\n**Question:** How does the Spark approach to streaming integrate with other Spark features?  \n**Answer:** Spark Streaming uses the same DataFrame API, allowing easy integration with other Spark functionality."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Real Time Deployment]($./08-Real-Time-Deployment )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on streaming ETL jobs?  \n**A:** Check out the Databricks blog post <a href=\"https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html\" target=\"_blank\">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a>\n\n**Q:** Where can I get more information on integrating Streaming and Kafka?  \n**A:** Check out the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\" target=\"_blank\">Structured Streaming + Kafka Integration Guide</a>\n\n**Q:** Where can I see a case study on an IoT pipeline using Spark Streaming?  \n**A:** Check out the Databricks blog post <a href=\"https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\" target=\"_blank\">Processing Data in Apache Kafka with Structured Streaming in Apache Spark 2.2</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"07-Streaming-Deployment","notebookId":1121904905107941},"nbformat":4,"nbformat_minor":0}

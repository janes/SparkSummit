{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Batch Deployment\n\nBatch inference is the most common way of deploying machine learning models.  This lesson introduces various strategies for deploying models using batch including pure Python, Spark, and on the JVM.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Explore batch deployment options\n - Predict on a Pandas DataFrame and save the results\n - Predict on a Spark DataFrame and save the results\n - Compare other batch deployment options"],"metadata":{}},{"cell_type":"markdown","source":["### Inference in Batch\n\nBatch deployment represents the vast majority of use cases for deploying machine learning models.<br><br>\n\n* This normally means running the predictions from a model and saving them somewhere for later use\n* For live serving, results are often saved to a database that will serve the saved prediction quickly  \n* In other cases, such as populating emails, they can be stored in less performant data stores such as a blob store\n\nWriting the results of your inference can be optimized in a number of ways...<br><br>  \n\n* For large sums of data, writes should be performed in parallel\n* **The access pattern for the saved predictions should also be kept in mind in how the data is written** \n  - For static files or data warehouses, partitioning speeds up data reads  \n  - For databases, indexing the database on the relevant query generally improves performance \n  - In either case, the index is working similar to an index in a book: it allows you to skip ahead to the relevant content"],"metadata":{}},{"cell_type":"markdown","source":["There are a few other considerations to ensure the accuracy of your model...<br><br>  \n\n* First is to make sure that your model matches expectations\n  - We'll cover this in further detail in the model drift section \n* Second is to **retrain your model on the entirety of your dataset**  \n  - A train/test split is a good method in tuning hyperparameters and estimating how the model will perform on unseen data\n  - Retraining the model on the entirety of your data ensures that you have as much information as possible factored into the model."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Inference in Pure Python\n\nInference in Python leverages the predict functionality of the machine learning package or MLflow wrapper."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nImport the data.  **Do not perform a train/test split.**\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> It is common to skip the train/test split in training a final model."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")\n\nX = df.drop([\"price\"], axis=1)\ny = df[\"price\"]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Train a final model"],"metadata":{}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=5)\nrf.fit(X, y)\n\npredictions = X.copy()\npredictions[\"prediction\"] = rf.predict(X)\n\nmse = mean_squared_error(y, predictions[\"prediction\"]) # This is on the same data the model was trained"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\nSave the results and partition by zipcode.  Note that zip code was indexed.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Should zip code be modeled as a continuous or categorical feature?"],"metadata":{}},{"cell_type":"code","source":["import os\n\npath = userhome + \"/ml-production/mlflow-model-training/batch-predictions/\"\n\ndbutils.fs.rm(path.replace(\"/dbfs\", \"dbfs:\"), True)\n\nfor i, partition in predictions.groupby(predictions[\"zipcode\"]):\n  dirpath = path + str(i)\n  print(\"Writing to {}\".format(dirpath))\n  os.makedirs(dirpath)\n  \n  partition.to_csv(dirpath + \"/predictions.csv\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Log the model and predictions."],"metadata":{}},{"cell_type":"code","source":["import mlflow.sklearn\nfrom sklearn.metrics import mean_squared_error\n\n# mlflow.set_experiment(experiment_name=experimentPath)\n\nwith mlflow.start_run(run_name=\"Final RF Model\") as run: \n  mlflow.sklearn.log_model(rf, \"random-forest-model\")\n  mlflow.log_metric(\"mse on training data\", mse)\n  \n  mlflow.log_artifacts(path, \"predictions\")\n  \n  run_info = run.info"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\n### Inference in Spark\n\nModels trained in various machine learning libraries can be applied at scale using Spark.  To do this, use `mlflow.pyfunc.spark_udf` and pass in the `SparkSession`, name of the model, and run id.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Using UDF's in Spark means that supporting libraries must be installed on every node in the cluster.  In the case of `sklearn`, this is installed in Databricks clusters by default.  With using other libraries, install them using the UI in order to ensure that they will work as a UDF."],"metadata":{}},{"cell_type":"markdown","source":["Create a Spark DataFrame from the Pandas DataFrame."],"metadata":{}},{"cell_type":"code","source":["XDF = spark.createDataFrame(X)\n\ndisplay(XDF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["MLflow easily produces a Spark user defined function (UDF).  This bridges the gap between Python environments and applying models at scale using Spark."],"metadata":{}},{"cell_type":"code","source":["pyfunc_udf = mlflow.pyfunc.spark_udf(spark, \"random-forest-model\", run_id=run_info.run_uuid)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["-sandbox\nApply the model as a standard UDF using the column names as the input to the function.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Python has an internal limit to the maximum number of arguments you can pass to a function.  The maximum number of features in a model applied in this way is therefore 255.  This limit will be changed in Python 3.7."],"metadata":{}},{"cell_type":"code","source":["predictionDF = XDF.withColumn(\"prediction\", pyfunc_udf(*X.columns))\n\ndisplay(predictionDF)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Other Deployment Options\n\nThere are a number of other common batch deployment options.  One common use case is going from a Python environment for training to a Java environment.  Here are a few tools:<br><br>\n\n - **An Easy Port to Java:** In certain models, such as linear regression, the coefficients of a trained model can be taken and implemented by hand in Java.  This can work with tree-based models as well\n - **Re-serializing for Java:** Since Python uses Pickle by default to serialize, a library like <a href=\"https://github.com/jpmml/jpmml-sklearn\" target=\"_blank\">jpmml-sklearn</a> can de-serialize `sklearn` libraries and re-serialize them for use in Java environments\n - **Leveraging Library Functionality:** Some libraries include the ability to deploy to Java such as <a href=\"https://github.com/dmlc/xgboost/tree/master/jvm-packages\" target=\"_blank\">xgboost4j</a>\n - **Containers:** Using containerized solutions are becoming increasingly popular since they offer the encapsulation and reliability offered by jars while offering better more deployment options than just the Java environment.\n \nFinally, <a href=\"http://mleap-docs.combust.ml/\" target=\"_blank\">MLeap</a> is a common, open source serialization format and execution engine for Spark, `sklearn`, and `TensorFlow`"],"metadata":{}},{"cell_type":"markdown","source":["## Review\n**Question:** What are the main considerations in batch deployments?  \n**Answer:** The following considerations help determine the best way to deploy batch inference results:\n* How the data will be queried\n* How the data will be written \n* The training and deployment environment\n* What data the final model is trained on\n\n**Question:** How can you optimize inference reads and writes?  \n**Answer:** Writes can be optimized by managing parallelism.  In Spark, this would mean managing the partitions of a DataFrame such that work is evenly distributed and you have the most efficient connections back to the target database.\n\n**Question:** How can I deploy models trained in Python in a Java environment?  \n**Answer:** There are a number of ways to do this.  It's not unreasonable to just export model coefficients or trees in a random forest and parse them in Java.  This works well as a minimum viable product.  You can also look at different libraries that can serialize models in a way that the JVM can make use of them.  `jpmml-sklearn` and `xgboost4j` are two examples of this.  Finally, you can re-implement Python libraries in Java if needed."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Streaming Deployment]($./07-Streaming-Deployment )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find more information on UDF's created by MLflow?  \n**A:** See the <a href=\"https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html\" target=\"_blank\">MLflow documentation for details</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"06-Batch-Deployment","notebookId":1121904905107219},"nbformat":4,"nbformat_minor":0}

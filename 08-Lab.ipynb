{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Lab 8: Querying from SageMaker vs from Parquet\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you:<br>\n - Deploy a model to SageMaker\n - Set up querying through a parquet file\n - Conduct time comparisons between the 2 methods"],"metadata":{}},{"cell_type":"code","source":["%run \"./../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Set up SageMaker\n\nRun the following cell to deploy our Airbnb model to SageMaker."],"metadata":{}},{"cell_type":"code","source":["import json\nimport os\nimport boto3\n\n# Set AWS credentials as environment variables\nos.environ[\"AWS_ACCESS_KEY_ID\"] = 'AKIAI4T2MLVBUB372FAA'\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = 'g1lSUmTtP2Y5TM4G3nryqg4TysUeKuJLKG0EYAZE' # READ ONLY ACCESS KEYS\nos.environ[\"AWS_DEFAULT_REGION\"] = 'us-west-2'\n\n\ndef query_endpoint_example(inputs, appName=\"airbnb-latest-0001\", verbose=True):\n  if verbose:\n    print(\"Sending batch prediction request with inputs: {}\".format(inputs))\n  client = boto3.session.Session().client(\"sagemaker-runtime\", \"us-west-2\")\n  \n  response = client.invoke_endpoint(\n      EndpointName=appName,\n      Body=json.dumps(inputs),\n      ContentType='application/json',\n  )\n  preds = response['Body'].read().decode(\"ascii\")\n  preds = json.loads(preds)\n  \n  if verbose:\n    print(\"Received response: {}\".format(preds))\n  return preds\n\ndef check_status(appName):\n  sage_client = boto3.client('sagemaker', region_name=\"us-west-2\")\n  endpoint_description = sage_client.describe_endpoint(EndpointName=appName)\n  endpoint_status = endpoint_description[\"EndpointStatus\"]\n  return endpoint_status\n\nprint(\"Application status is: {}\".format(check_status(appName=\"airbnb-latest-0001\")))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Load in airbnb data to use as inputs for our queries.\n\nReference the helper function in the 08-Real-Time-Deployment notebook to complete the `random_n_samples_sagemaker` function so that it connects to the `sagemaker-runtime` client and sends the record in the appropriate JSON format."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n\ndef random_n_samples_sagemaker(n, df=X_train, verbose=False):\n  dfShape = X_train.shape[0]\n  samples = []\n  \n  for i in range(n):\n    sample = X_train.iloc[[random.randint(0, dfShape-1)]].values\n    samples.append(sample.flatten().tolist())\n  \n  return query_endpoint_example(samples, appName=\"airbnb-latest-0001\", verbose=verbose)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Check that the output of `random_n_samples_sagemaker` is what you expect."],"metadata":{}},{"cell_type":"code","source":["random_n_samples_sagemaker(5, verbose=False)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Using Parquet File\n\nRead in the Airbnb parquet file as a Spark DataFrame."],"metadata":{}},{"cell_type":"code","source":["prediction_df = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/prediction.parquet\") \ndisplay(prediction_df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Complete the following helper function `random_n_samples_parquet` to return `n` random queries from `predictions_df`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col\n\ndef random_n_samples_parquet(n, df=prediction_df):\n\n  # get n ids to query using\n  id_list = df.limit(n).select(\"id\").collect()\n  query_ids = [i[0] for i in id_list]\n  \n  # get prediction of each row of 'query_ids'\n  preds = []\n  for i in query_ids:\n    pred = (df\n      .filter(col(\"id\") == i)\n      .select(\"prediction\")\n      .first()\n    )[0]\n    preds.append(pred)\n    \n  return preds"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Check that the output of `random_n_samples_parquet` is what you expect."],"metadata":{}},{"cell_type":"code","source":["random_n_samples_parquet(5)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Time Comparisons\n\nLet's compare the time it takes to get the result of a single query from SageMaker and from a parquet file."],"metadata":{}},{"cell_type":"code","source":["batch_size = 1"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%timeit -n5 random_n_samples_sagemaker(batch_size)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%timeit -n5 random_n_samples_parquet(batch_size)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Using the read in parquet file is faster!\n\nBut what if we want to ask multiple queries? Increase `batch_size` up to 30 and see which method is faster with a larger number of queries."],"metadata":{}},{"cell_type":"code","source":["batch_size = 30"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%timeit -n5 random_n_samples_sagemaker(batch_size)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%timeit -n5 random_n_samples_parquet(batch_size)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Which performed better?  What are the trade-offs between using the two in production?"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"08-Lab","notebookId":1121904905106399},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Drift Monitoring\n\nMonitoring models over time entails safeguarding against drift in model performance.  In this lesson, you explore solutions to drift and implement a basic retraining method and two ways of dynamically monitoring drift.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Develop a model monitoring strategy for data drift and model drift\n - Create a non-adaptive drift solution that periodically retrains a model\n - Create an adaptive solution that detects drift using an error threshold \n - Create an adaptive solution that detects drift using more complex error statistics"],"metadata":{}},{"cell_type":"markdown","source":["### Drift\n\nThe majority of machine learning solutions...<br><br>\n\n* Assume that data is generated according to a stationary probability distribution\n* Most datasets involving human activity evolve over time\n* A **context** is a set of samples from a distribution that are generated by a stationary function\n* When this context changes, we can observe a change in the probability distribution\n\n**Monitoring for drift involves monitoring for this change of context, often quantified by an increase in the error rate.**"],"metadata":{}},{"cell_type":"markdown","source":["There are two basic categories of solutions...<br><br>\n\n* The first adapts the learner regardless of whether drift has occurred\n  - This solution would include some notion of memory\n  - Filtering for a certain time window (e.g. all data from the past week) \n  - Using weighted examples (e.g. this month's data is weighted twice as important as last month's)\n  - The main challenge with this option is choosing the best window or filtering method\n* Without the ability to actively detect drift, it somewhat arbitrarily selects that threshold\n\nThe second solution adapts the learner when it detects drift...<br><br>  \n\n* An adaptive solution is ideal since it can detect the optimal time for retraining \n* Will make more data available to the learner, improving model performance"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/drift.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nThis is an example of an adaptive learner...<br><br>  \n\n* It monitors incoming data coming incrementally or in new batches for an increase in error\n* Once that error reaches a certain threshold, a warning is issued\n* Once it reaches a second threshold, the data is said to be in a new context\n  - This triggers a retraining of the model on the data between the first and second thresholds\n* The thresholds in this example are set using the 95% and 99% confidence intervals."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Basic Retraining Solution\n\nBasic solutions to the issue of drift can include the following:<br><br>\n\n - Retrain the model periodically on all new and historical data\n - Retrain the model on a known window of data (e.g. the last week of data)\n - Retrain the model while weighing more recent data more strongly\n\nTo illustrate model retraining, look at code that changes over time to get a sense for how it affects model error."],"metadata":{}},{"cell_type":"markdown","source":["Set up a series of predictions draw from 3 different distributions."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\ndistribution1 = np.stack([np.random.normal(loc=0, scale=1, size=1000), np.random.random(1000), np.ones(1000)*1]).T\ndistribution2 = np.stack([np.random.normal(loc=2, scale=1.5, size=1000), np.random.random(1000), np.ones(1000)*2]).T\ndistribution3 = np.stack([np.random.normal(loc=5, scale=1, size=1000), np.random.random(1000), np.ones(1000)*3]).T\n\ndf = pd.DataFrame(np.vstack([distribution1, distribution2, distribution3]), columns=[\"y\", \"random-x\", \"distribution\"])\ndf[\"time\"] = df.index\n\ndf"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Visualize the changing distribution over time."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nax.scatter(df['time'], df['y'], c=df['distribution'], alpha=.2)\nax.set_title(\"Changing Distribution over Time\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Value\")\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["We can see that the average and spread of the data changes over time across our three distributions.  Write a helper function to train models on subsets of the data."],"metadata":{}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\ndef return_mse(df, distribution=None):\n  if distribution:\n    subset_df = df[df['distribution'] == distribution]\n  else:\n    subset_df = df.copy()\n    \n  trained_model = RandomForestRegressor(n_estimators=100, random_state=42).fit(subset_df['random-x'].values.reshape(-1, 1), subset_df['y'])\n  mse = mean_squared_error(subset_df['y'], trained_model.predict(subset_df['random-x'].values.reshape(-1, 1)))\n  return mse, trained_model\n  \nreturn_mse(df)[0]"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["See how the different subsets compare."],"metadata":{}},{"cell_type":"code","source":["print(\"MSE for model trained on all data: {}\".format(return_mse(df)[0]))\nprint(\"MSE for model trained on distribution 1: {}\".format(return_mse(df, 1)[0]))\nprint(\"MSE for model trained on distribution 2: {}\".format(return_mse(df, 2)[0]))\nprint(\"MSE for model trained on distribution 3: {}\".format(return_mse(df, 3)[0]))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["What is the optimal window to retrain the model?"],"metadata":{}},{"cell_type":"markdown","source":["### Detecting by Error Threshold\n\nMonitoring for error threshold is a more dynamic way of handling model drift.  In practice, this is one of the more common ways of handling drift where a data scientist is alerted at the point the model error rises above a certain level.\n\nCreate a process for alerting when an error rate increases above .5"],"metadata":{}},{"cell_type":"markdown","source":["Create the error threshold."],"metadata":{}},{"cell_type":"code","source":["mse, model = return_mse(df, 1)\nmse_threshold = mse * 1.5"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Create a helper function to test a model."],"metadata":{}},{"cell_type":"code","source":["def test_for_drift(df, threshold, distribution=None):\n  if return_mse(df, distribution)[0] > threshold:\n    return \"The model has drifted\"\n  else:\n    return \"The model has not drifted\""],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Test the first distribution."],"metadata":{}},{"cell_type":"code","source":["test_for_drift(df, mse_threshold, distribution=1)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Test the next distribution."],"metadata":{}},{"cell_type":"code","source":["test_for_drift(df, mse_threshold, distribution=2)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Detecting Drift by Statistical Distribution\n\nAs we saw above, choosing the best window to retrain our model is difficult, and is often done somewhat randomly.  The best window depends on a number of issues, especially on how abruptly we expect drift to happen.\n\nA more rigorous way of addressing drift is to first detect drift using statistical methods.  This could be as simple as monitoring model error.  The package <a href=\"https://scikit-multiflow.github.io/scikit-multiflow/skmultiflow.drift_detection.html#module-skmultiflow.drift_detection\" target=\"_blank\">`skmultiflow` has some good options for this.</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nTry the DDM method.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/drift.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nThe detection threshold is calculated in function of two statistics, obtained when `(pi + si)` is minimum:\n\n * `pmin`: The minimum recorded error rate.\n * `smin`: The minimum recorded standard deviation.\n\nAt instant `i`, the detection algorithm uses:\n\n * `pi`: The error rate at instant i.\n * `si`: The standard deviation at instant i.\n\nThe default conditions for entering the warning zone and detecting change are as follows:\n\n * if `pi + si >= pmin + 2 * smin` -> Warning zone\n * if `pi + si >= pmin + 3 * smin` -> Change detected\n \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> <a href=\"https://github.com/scikit-multiflow/scikit-multiflow/blob/ddf104437132f3b75f3fa6d30195e09d7bcb3231/src/skmultiflow/drift_detection/ddm.py#L5\" target=\"_blank\">See the implementation here.</a>"],"metadata":{}},{"cell_type":"markdown","source":["Define the DDM object.  Set the parameters for the minimum number of instances before alerting a change as well as the two thresholds."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom skmultiflow.drift_detection import DDM\n\nddm = DDM(min_num_instances=100, warning_level=1.98, out_control_level=2)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Simulate a data stream as a uniform distribution of 1's and 0's.  These numbers represent misclassification error where a 1 is a misclassified prediction."],"metadata":{}},{"cell_type":"code","source":["data_points=8000\ndata_stream = np.random.randint(2, size=data_points)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Simulate a change in context by making the algorithm perform poorly at different points."],"metadata":{}},{"cell_type":"code","source":["for i in range(999, 1500):\n    data_stream[i] = 1\n\nfor i in range(3500, 6500):\n    data_stream[i] = 1\n\nfor i in range(7500, 8000):\n    data_stream[i] = 1"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Now run the simulated errors through DDM.  Alert if a warning zone or change zone has been detected."],"metadata":{}},{"cell_type":"code","source":["for i in range(data_points):\n    ddm.add_element(data_stream[i])\n    if ddm.detected_warning_zone():\n       print(\"Warning zone detected at index {} data {}\".format(i, data_stream[i]))\n    if ddm.detected_change():\n       print(\"Change detected at index {}\".format(i))\n#        ddm.reset()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Did the algorithm detect change where you thought it would?"],"metadata":{}},{"cell_type":"markdown","source":["## Review\n\n**Question:** Why do some of the common assumptions of machine learning not apply to real world datasets?  \n**Answer:** On large assumption in machine learning is the existence of static data created by a static distribution.  The reality is that data changes over time when the underlying mechanism of that change is not fully modeled.  Real world machine learning models have to be able to handle \"concept drift\" where the context in which the model was changed evolves over time.  This can pose many challenges, one being in recommender systems that want to recommend to different modalities of a user.  For instance, one of those modalities might be nostalgia, which would require the use of older data.\n\n**Question:** What are the pros and cons of using a window or function that considers recent data more strongly?  \n**Answer:** The main benefit of this approach is that it will adapt a model over time.  The downside is that it is difficult to know who to set the parameters of that window.  We know with statistical certainty that models perform better with more data when that data originates from the same probability distribution.  However, knowing when that context has changed is challenging so knowing what data to use when retraining a model is non-trivial.\n\n**Question:** What is the best way of knowing when to retrain a model?  \n**Answer:** The easiest solution is to set up an alerting mechanism for when a model's error begins to slip.  The naive implementation of this might look for a 10% slip in error before raising an alarm.  A more rigorous approach would look to quantify the shift in context.  These approaches often look at the confidence intervals for a given outcome such as the label.  When a threshold is reached, it is said that the data is originating from a new context.  These adaptive solutions are largely understudied, so academic research and newer libraries are the best place to source these solutions."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Alerting]($./10-Alerting )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** What are libraries for handling drift?  \n**A:** Check out <a href=\"https://moa.cms.waikato.ac.nz/\" target=\"_blank\">MOA</a>, <a href=\"https://scikit-multiflow.github.io/\" target=\"_blank\">scikit-multiflow</a> and <a href=\"https://www.tensorflow.org/tfx/data_validation/get_started\" target=\"_blank\">TensorFlow Data Validation.</a>\n\n\n**Q:** What's a good general introduction to drift?  \n**A:** Check out <a href=\"https://en.wikipedia.org/wiki/Concept_drift\" target=\"_blank\">the Wikipedia article on it.</a>\n\n**Q:** What are good academic resources for drift?  \n**A:** Check out the papers <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.1833&rep=rep1&type=pdf\" target=\"_blank\">Learning with Drift Detection</a>, <a href=\"https://www.win.tue.nl/~mpechen/publications/pubs/CD_applications15.pdf\" target=\"_blank\">An overview of concept drift applications</a>, and <a href=\"https://arxiv.org/pdf/1504.01044.pdf\" target=\"_blank\">Concept Drift Detection for Streaming Data</a>.\n\n**Q:** How can I classify different types of drift?  \n**A:** Check out the paper <a href=\"https://rtg.cis.upenn.edu/cis700-2019/papers/dataset-shift/dataset-shift-terminology.pdf\" target=\"_blank\">A unifying view on dataset shift in classification</a>."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"09-Drift-Monitoring","notebookId":1121904905107067},"nbformat":4,"nbformat_minor":0}

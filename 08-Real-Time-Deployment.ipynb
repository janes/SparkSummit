{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Real Time Deployment\n\nWhile real time deployment represents a smaller share of the deployment landscape, many of these deployments represent high value tasks.  This lesson surveys real time deployment options ranging from proofs of concept to both custom and managed solutions with a focus on RESTful services.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Survey the landscape of real time deployment options\n - Prototype a RESTful service using MLflow\n - Walk through the deployment of REST endpoint using SageMaker\n - Query a REST endpoint for inference using individual records and batch requests"],"metadata":{}},{"cell_type":"markdown","source":["### The Why and How of Real Time Deployment\n\nReal time inference is...<br><br>\n\n* Generating predictions for a small number of records with fast results (e.g. results in milliseconds)\n* The first question to ask when considering real time deployment is: do I need it?  \n  - It represents a minority of machine learning inference use cases \n  - Is one of the more complicated ways of deploying models\n  - That being said, domains where real time deployment is often needed are often of great business value.  \n  \nDomains needing real time deployment include...<br><br>\n\n - Financial services (especially with fraud detection)\n - Mobile\n - Adtech"],"metadata":{}},{"cell_type":"markdown","source":["There are a number of ways of deploying models...<br><br>\n\n* Many use REST\n* For basic prototypes, MLflow can act as a development deployment server\n  - The MLflow implementation is backed by the Python library Flask\n  - *This is not intended to for production environments*\n\nFor production RESTful deployment, there are two main options...<br><br>\n\n* A managed solution \n  - Azure ML\n  - SageMaker\n* A custom solution  \n  - Involve deployments using a range of tools\n  - Often using Docker, Kubernetes, and Elastic Beanstalk\n* One of the crucial elements of deployment in containerization\n  - Software is packaged and isolated with its own application, tools, and libraries\n  - Containers are a more lightweight alternative to virtual machines\n\nFinally, embedded solutions are another way of deploying machine learning models, such as storing a model on IoT devices for inference."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["-sandbox\n### Prototyping with MLflow\n\nMLflow offers <a href=\"https://www.mlflow.org/docs/latest/models.html#pyfunc-deployment\" target=\"_blank\">a Flask-backed deployment server for development.</a>"],"metadata":{}},{"cell_type":"markdown","source":["Build a basic model. This model will always predict 5."],"metadata":{}},{"cell_type":"code","source":["import mlflow\nimport mlflow.pyfunc\n\nclass TestModel(mlflow.pyfunc.PythonModel):\n  \n  def predict(self, context, input_df):\n    return 5\n  \nartifact_path=\"pyfunc-model\"\n\nwith mlflow.start_run():\n  mlflow.pyfunc.log_model(artifact_path=artifact_path, python_model=TestModel())\n  \n  run_id = mlflow.active_run().info.run_uuid"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\nIn its current development, the server is only accessible through the CLI using `mlflow pyfunc serve`.  This will change in future development.  In the meantime, we can work around this using `click`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Models can be served in this way in other languages as well."],"metadata":{}},{"cell_type":"code","source":["from multiprocessing import Process\n\nserver_port_number = 6501\n\ndef run_server():\n  try:\n    import mlflow.pyfunc.cli\n    from click.testing import CliRunner\n    \n    CliRunner().invoke(mlflow.pyfunc.cli.commands, \n                       ['serve', \n                        \"--model-path\", artifact_path, \n                        \"--run-id\", run_id, \n                        \"--port\", server_port_number, \n                        \"--host\", \"127.0.0.1\", \n                        \"--no-conda\"])\n  except Exception as e:\n    print(e)\n\np = Process(target=run_server) # Run as a background process\np.start()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Create an input for our REST input."],"metadata":{}},{"cell_type":"code","source":["import json\nimport pandas as pd\n\ninput_df = pd.DataFrame([0])\ninput_json = input_df.to_json(orient='split')\n\ninput_json"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Perform a POST request against the endpoint."],"metadata":{}},{"cell_type":"code","source":["import requests\n\nheaders = {'Content-type': 'application/json'}\nurl = \"http://localhost:{port_number}/invocations\".format(port_number=server_port_number)\n\nresponse = requests.post(url=url, headers=headers, data=input_json)\n\nprint(response)\nprint(response.text)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Do the same in bash."],"metadata":{}},{"cell_type":"code","source":["%sh (echo -n '{\"columns\":[0],\"index\":[0],\"data\":[[0]]}') | curl -H \"Content-Type: application/json\" -d @- http://127.0.0.1:6501/invocations"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Managed Service Walk-through\n\nChoose one of the following:<br><br>\n\n* [A walk-through of deployment to Azure ML]($./Extras/AzureML-Deployment ) and the corresponding <a href=\"https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml\" target=\"_blank\">MLflow docs</a>\n* [A walk-through of deployment to AWS SageMaker]($./Extras/SageMaker-Deployment ) and the corresponding <a href=\"https://www.mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-amazon-sagemaker\" target=\"_blank\">MLflow docs</a>"],"metadata":{}},{"cell_type":"markdown","source":["### SageMaker\n\nThis example assumes that the model was already deployed to SageMaker.  See the walk-through above in case you missed it.  Now let's look at how we'll query that REST endpoint."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nFirst set AWS keys as environment variables.  **This is not a best practice since this is not the most secure way of handling credentials.**  This works in our case sense the keys have a very limited policy associated with them.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the <a href=\"hhttps://docs.azuredatabricks.net/api/latest/secrets.html#id1\" target=\"_blank\">Secrets API</a> and <a href=\"https://docs.databricks.com/administration-guide/cloud-configurations/aws/iam-roles.html\" target=\"_blank\">IAM roles</a> for more secure ways of storing keys."],"metadata":{}},{"cell_type":"code","source":["import os\n\n# Set AWS credentials as environment variables\nos.environ[\"AWS_ACCESS_KEY_ID\"] = 'AKIAI4T2MLVBUB372FAA'\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = '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' # READ ONLY ACCESS KEYS\nos.environ[\"AWS_DEFAULT_REGION\"] = 'us-west-2'"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Use `boto3`, the library for interacting with AWS in Python, to check the application status."],"metadata":{}},{"cell_type":"code","source":["import boto3\n\ndef check_status(appName):\n  sage_client = boto3.client('sagemaker', region_name=\"us-west-2\")\n  endpoint_description = sage_client.describe_endpoint(EndpointName=appName)\n  endpoint_status = endpoint_description[\"EndpointStatus\"]\n  return endpoint_status\n\nprint(\"Application status is: {}\".format(check_status(appName=\"airbnb-latest-0001\")))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Import the Airbnb dataset and pull out the first record."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"/dbfs/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\")\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\nquery_input = X_train.iloc[[0]].values.flatten().tolist()\n\nprint(\"Using input vector: {}\".format(query_input))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Define a helper function that connects to the `sagemaker-runtime` client and sends the record in the appropriate JSON format."],"metadata":{}},{"cell_type":"code","source":["import json\n\ndef query_endpoint_example(inputs, appName=\"airbnb-latest-0001\", verbose=True):\n  if verbose:\n    print(\"Sending batch prediction request with inputs: {}\".format(inputs))\n  client = boto3.session.Session().client(\"sagemaker-runtime\", \"us-west-2\")\n  \n  response = client.invoke_endpoint(\n      EndpointName=appName,\n      Body=json.dumps(inputs),\n      ContentType='application/json',\n  )\n  preds = response['Body'].read().decode(\"ascii\")\n  preds = json.loads(preds)\n  \n  if verbose:\n    print(\"Received response: {}\".format(preds))\n  return preds"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Query the endpoint."],"metadata":{}},{"cell_type":"code","source":["prediction = query_endpoint_example(inputs=[query_input])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Now try the same but by using more than just one record.  Create a helper function to query the endpoint with a number of random samples."],"metadata":{}},{"cell_type":"code","source":["def random_n_samples(n, df=X_train, verbose=False):\n  dfShape = X_train.shape[0]\n  samples = []\n  \n  for i in range(n):\n    sample = X_train.iloc[[random.randint(0, dfShape-1)]].values\n    samples.append(sample.flatten().tolist())\n  \n  return query_endpoint_example(samples, appName=\"airbnb-latest-0001\", verbose=verbose)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Test this using 10 samples.  The payload for SageMaker can be 1 or more samples."],"metadata":{}},{"cell_type":"code","source":["random_n_samples(10, verbose=True)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Compare the times between payload sizes.  **Notice how sending more records at a time reduces the time to prediction for each individual record.**"],"metadata":{}},{"cell_type":"code","source":["%timeit -n5 random_n_samples(100)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%timeit -n5 random_n_samples(1)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## Review\n\n - Survey the landscape of real time deployment options\n - Prototype a RESTful service using MLflow\n - Walk through the deployment of REST endpoint using SageMaker\n - Query a REST endpoint for inference using individual records and batch requests\n\n**Question:** What are the best tools for real time deployment?  \n**Answer:** This depends largely on the desired features.  The main tools to consider are a way to containerize code and either a REST endpoint or an embedded model.  This covers the vast majority of real time deployment options.\n\n**Question:** What are the best options for RESTful services?  \n**Answer:** The major cloud providers all have their respective deployment options.  In the Azure environment, Azure ML manages deployments using Docker images.  AWS SageMaker does the same.  This provides a REST endpoint that can be queried by various elements of your infrastructure.\n\n**Question:** What factors influence REST deployment latency?  \n**Answer:** Response time is a function of a few factors.  Batch predictions should be used when needed since it improves throughput by lowering the overhead of the REST connection.  Geo-location is also an issue, as is server load.  This can be handled by geo-located deployments and load balancing with more resources."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Drift Monitoring]($./09-Drift-Monitoring )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more information on MLflow's `pyfunc`?  \n**A:** Check out <a href=\"https://www.mlflow.org/docs/latest/models.html#pyfunc-deployment\" target=\"_blank\">the MLflow documentation</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"08-Real-Time-Deployment","notebookId":1121904905105844},"nbformat":4,"nbformat_minor":0}

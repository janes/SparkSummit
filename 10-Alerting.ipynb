{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Alerting\n\nAlerting allows you to announce the progress of different applications, which becomes increasingly important in automated production systems.  In this lesson, you explore basic alerting strategies using email and REST integration with tools like Slack.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Explore the alerting landscape\n - Walk through basic email alerting using Databricks Jobs\n - Create a basic REST alert integrated with Slack\n - Create a more complex REST alert for Spark jobs using `SparkListener`"],"metadata":{}},{"cell_type":"markdown","source":["### The Alerting Landscape\n\nThere are a number of different alerting tools with various levels of sophistication...<br><br>\n\n* PagerDuty has risen to be one of the most popular tools for monitoring production outages\n  - It allows for the escalation of issues across a team with alerts including text messages and phone calls\n* Slack\n* Twilio   \n* Email alerts\n\nMost alerting frameworks allows for custom alerting done through REST integration\n\nOne additional helpful tool for Spark workloads....<br><br> \n\n* Is the `SparkListener`\n* It can perform custom logic on various Cluster actions"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Setting Basic Alerts\n\nCreate a basic alert using a Slack endpoint."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nDefine a Slack webhook.  This has been done for you.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Define your own Slack webhook <a href=\"https://api.slack.com/incoming-webhooks#getting-started\" target=\"_blank\">Using these 4 steps.</a><br>\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This same approach applies to PagerDuty as well."],"metadata":{}},{"cell_type":"code","source":["webhookMLProductionAPIDemo = \"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Send a test message and check Slack."],"metadata":{}},{"cell_type":"code","source":["def postToSlack(webhook, content):\n  import requests\n  from string import Template\n  t = Template('{\"text\": \"${content}\"}')\n  \n  response = requests.post(webhook, data=t.substitute(content=content), headers={'Content-Type': 'application/json'})\n  \npostToSlack(webhookMLProductionAPIDemo, \"This is my post from Python\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Do the same thing using Scala.  This involves a bit more boilerplate and a different library."],"metadata":{}},{"cell_type":"code","source":["%scala\n\ndef postToSlack(webhook:String, content:String):Unit = {\n  import org.apache.http.entity._\n  import org.apache.http.impl.client.{HttpClients}\n  import org.apache.http.client.methods.HttpPost\n\n  val client = HttpClients.createDefault()\n  val httpPost = new HttpPost(webhook)\n  \n  val payload = s\"\"\"{\"text\": \"${content}\"}\"\"\"\n\n  val entity = new StringEntity(payload)\n  httpPost.setEntity(entity)\n  httpPost.setHeader(\"Accept\", \"application/json\")\n  httpPost.setHeader(\"Content-type\", \"application/json\")\n\n  val response = client.execute(httpPost)\n  client.close()\n}\n\nval webhook = \"https://hooks.slack.com/services/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\"\n\npostToSlack(webhookMLProductionAPIDemo, \"This is my post from Scala\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Now you can easily integrate custom logic back to Slack."],"metadata":{}},{"cell_type":"code","source":["mse = .45\n\npostToSlack(webhookMLProductionAPIDemo, \"The newly trained model MSE is now {}\".format(mse))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\n### Using a `SparkListener`\n\nA custom `SparkListener` allows for custom actions taken on cluster activity.  **This API is only available in Scala.**  Take a look at the following code.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener\" target=\"_blank\">See the `SparkListener` docs here.</a>"],"metadata":{}},{"cell_type":"code","source":["%scala\n// Package in a notebook helps to ensure a proper singleton\npackage com.databricks.academy\n\nobject SlackNotifyingListener extends org.apache.spark.scheduler.SparkListener {\n  import org.apache.spark.scheduler._\n\n  val webhook = \"https://hooks.slack.com/services/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\"\n  \n  def postToSlack(message:String):Unit = {\n    import org.apache.http.entity._\n    import org.apache.http.impl.client.{HttpClients}\n    import org.apache.http.client.methods.HttpPost\n\n    val client = HttpClients.createDefault()\n    val httpPost = new HttpPost(webhook)\n\n    val content = \"\"\"{ \"text\": \"%s\" }\"\"\".format(message)\n    \n    val entity = new StringEntity(content)\n    httpPost.setEntity(entity)\n    httpPost.setHeader(\"Accept\", \"application/json\")\n    httpPost.setHeader(\"Content-type\", \"application/json\")\n\n    val response = client.execute(httpPost)\n    client.close()\n  }\n  \n  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n    postToSlack(\"Called when the application ends\")\n  }\n\n  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = {\n    postToSlack(\"Called when the application starts\")\n  }\n\n  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = {\n    postToSlack(\"Called when a new block manager has joined\")\n  }\n\n  override def onBlockManagerRemoved(blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = {\n    postToSlack(\"Called when an existing block manager has been removed\")\n  }\n\n  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = {\n    postToSlack(\"Called when the driver receives a block update info.\")\n  }\n\n  override def onEnvironmentUpdate(environmentUpdate: SparkListenerEnvironmentUpdate): Unit = {\n    postToSlack(\"Called when environment properties have been updated\")\n  }\n\n  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {\n    postToSlack(\"Called when the driver registers a new executor.\")\n  }\n\n  override def onExecutorBlacklisted(executorBlacklisted: SparkListenerExecutorBlacklisted): Unit = {\n    postToSlack(\"Called when the driver blacklists an executor for a Spark application.\")\n  }\n\n  override def onExecutorBlacklistedForStage(executorBlacklistedForStage: SparkListenerExecutorBlacklistedForStage): Unit = {\n    postToSlack(\"Called when the driver blacklists an executor for a stage.\")\n  }\n\n  override def onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = {\n    // This one is a bit on the noisy side so I'm pre-emptively killing it\n    // postToSlack(\"Called when the driver receives task metrics from an executor in a heartbeat.\")\n  }\n\n  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = {\n    postToSlack(\"Called when the driver removes an executor.\")\n  }\n\n  override def onExecutorUnblacklisted(executorUnblacklisted: SparkListenerExecutorUnblacklisted): Unit = {\n    postToSlack(\"Called when the driver re-enables a previously blacklisted executor.\")\n  }\n\n  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {\n    postToSlack(\"Called when a job ends\")\n  }\n\n  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n    postToSlack(\"Called when a job starts\")\n  }\n\n  override def onNodeBlacklisted(nodeBlacklisted: SparkListenerNodeBlacklisted): Unit = {\n    postToSlack(\"Called when the driver blacklists a node for a Spark application.\")\n  }\n\n  override def onNodeBlacklistedForStage(nodeBlacklistedForStage: SparkListenerNodeBlacklistedForStage): Unit = {\n    postToSlack(\"Called when the driver blacklists a node for a stage.\")\n  }\n\n  override def onNodeUnblacklisted(nodeUnblacklisted: SparkListenerNodeUnblacklisted): Unit = {\n    postToSlack(\"Called when the driver re-enables a previously blacklisted node.\")\n  }\n\n  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n    postToSlack(\"Called when other events like SQL-specific events are posted.\")\n  }\n\n  override def onSpeculativeTaskSubmitted(speculativeTask: SparkListenerSpeculativeTaskSubmitted): Unit = {\n    postToSlack(\"Called when a speculative task is submitted\")\n  }\n\n  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {\n    postToSlack(\"Called when a stage completes successfully or fails, with information on the completed stage.\")\n  }\n\n  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {\n    postToSlack(\"Called when a stage is submitted\")\n  }\n\n  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {\n    postToSlack(\"Called when a task ends\")\n  }\n\n  override def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult): Unit = {\n    postToSlack(\"Called when a task begins remotely fetching its result (will not be called for tasks that do not need to fetch the result remotely).\")\n  }\n\n  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {\n    postToSlack(\"Called when a task starts\")\n  }\n\n  override def onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit = {\n    postToSlack(\"Called when an RDD is manually unpersisted by the application\")\n  }\n}"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Register this Singleton as a `SparkListener`"],"metadata":{}},{"cell_type":"code","source":["%scala\nsc.addSparkListener(com.databricks.academy.SlackNotifyingListener)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Now run a basic DataFrame operation and observe the results in Slack."],"metadata":{}},{"cell_type":"code","source":["%scala\nspark.read\n  .option(\"header\", true)\n  .option(\"inferSchema\", true)\n  .parquet(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet\")\n  .count"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["This will also work back in Python."],"metadata":{}},{"cell_type":"code","source":["(spark.read\n  .option(\"header\", True)\n  .option(\"inferSchema\", True)\n  .parquet(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet\")\n  .count()\n)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["When you're done, remove the listener."],"metadata":{}},{"cell_type":"markdown","source":["## Review\n**Question:** What are the most common alerting tools?  \n**Answer:** PagerDuty tends to be the tool most used in production environments.  SMTP servers emailing alerts are also popular, as is Twilio for text message alerts.  Slack webhooks and bots can easily be written as well.\n\n**Question:** How can I write custom logic to monitor Spark?  \n**Answer:** The `SparkListener` API is only exposed in Scala.  This allows you to write custom logic based on your cluster activity."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Delta Time Travel]($./11-Delta-Time-Travel )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find the alerting tools mentioned in this lesson?  \n**A:** Check out <a href=\"https://www.twilio.com\" target=\"_blank\">Twilio</a> and <a href=\"https://www.pagerduty.com\" target=\"_blank\">PagerDuty</a>."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"10-Alerting","notebookId":1121904905107890},"nbformat":4,"nbformat_minor":0}

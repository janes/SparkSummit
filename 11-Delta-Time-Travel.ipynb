{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Use Databricks Delta Time Travel and MLflow to Analyze Power Plant Data\n\nIn this notebook, we \n0. Stream power plant data to a Databricks Delta table\n0. Train our model on a current version of our data\n0. Post some results to MLflow\n0. Rewind to an older version of our data\n0. Re-train our model on an older version of our data\n0. Evaluate our (rewound) data \n0. Make predictions on the streaming data\n\nThe focus isn't necessarily on Machine Learning here, but, it is to show you how we may integrate the latest Databricks features in Machine Learning."],"metadata":{}},{"cell_type":"markdown","source":["Our schema definition from UCI appears below:\n\n- AT = Atmospheric Temperature [1.81-37.11]Â°C\n- V = Exhaust Vaccum Speed [25.36-81.56] cm Hg\n- AP = Atmospheric Pressure in [992.89-1033.30] milibar\n- RH = Relative Humidity [0-100]%\n- PE = Power Output [420.26-495.76] MW\n\nPE is our label or target. This is the value we are trying to predict given the measurements.\n\n*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["## Before we get started \n\nThis notebook makes use of a service called MLflow (more on this later).\n\nIn order to use this service we need to attach the following library to your cluster.\n\nPyPI: `mlflow==0.9.0`\n\n\nOnce the library is appropriatly attached, you can run the following import statement to confirm that your cluster is properly configured:"],"metadata":{}},{"cell_type":"code","source":["import mlflow"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup-08\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Slow Stream of Files\n\nOur stream source is a repository of many small files."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, DoubleType\n\ndataPath = \"/mnt/training/power-plant/streamed.parquet\"\nexperimentPath = \"/Users/\" + username + \"/delta-experiment\"\n\ndataSchema = StructType([\n  StructField(\"AT\", DoubleType(), True),\n  StructField(\"V\", DoubleType(), True),\n  StructField(\"AP\", DoubleType(), True),\n  StructField(\"RH\", DoubleType(), True),\n  StructField(\"PE\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["initialDF = (spark\n  .readStream                        # Returns DataStreamReader\n  .option(\"maxFilesPerTrigger\", 1)   # Force processing of only 1 file per trigger \n  .schema(dataSchema)                # Required for all streaming DataFrames\n  .parquet(dataPath) \n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Append to a Databricks Delta Table\n\nUse this to create `powerTable`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\nimport datetime\n\nwritePath = basePath + \"/output.parquet\"   # A subdirectory for our output\ncheckpointPath = basePath + \"/checkpoint\"  # A subdirectory for our checkpoint & W-A logs\nnow = datetime.datetime.now()\n\npowerTable = \"powerTable\""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["streamingQuery = (initialDF                                  # Start with our \"streaming\" DataFrame\n  .writeStream                                               # Get the DataStreamWriter\n  .trigger(processingTime=\"3 seconds\")                       # Configure for a 3-second micro-batch\n  .queryName(\"stream-1p\")                                    # Specify Query Name\n  .format(\"delta\")                                           # Specify the sink type, a Parquet file\n  .option(\"timestampAsOf\", now)                              # Timestamp the stream in the form of string that can be converted to TimeStamp\n  .outputMode(\"append\")                                      # Write only new data to the \"file\"\n  .option(\"checkpointLocation\", checkpointPath)              # Specify the location of checkpoint files & W-A logs\n  .table(powerTable)\n)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Cell below is to keep the stream running in case we do a RunAll"],"metadata":{}},{"cell_type":"code","source":["untilStreamIsReady(\"stream-1p\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Create a DataFrame out of the Delta stream so we can get a scatterplot.\n\nThis will be a \"snapshot\" of the data at an instant in time, so, a static table."],"metadata":{}},{"cell_type":"code","source":["staticPowerDF = spark.table(powerTable)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display( spark.sql(\"SELECT count(*) FROM {}\".format(powerTable)) )"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["##Use Scatter Plot show intution\n\nLet's plot `PE` versus other fields to see if there are any relationships.\n\nYou can toggle between fields by adjusting Plot Options.\n\nCouple observations\n* It looks like there is strong linear correlation between Atmospheric Temperature and Power Output\n* Maybe a bit of correlation between Atmospheric Pressure and Power Output"],"metadata":{}},{"cell_type":"code","source":["display(staticPowerDF)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["-sandbox\n##Train LR Model on Static DataFrame\n\n0. Split `staticPowerDF` into training and test set\n0. Use all features: AT, AP, RH and V\n0. Reshape training set\n0. Do linear regression\n0. Predict power output (PE)\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Data is changing underneath us"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Split DataFrame into test/train sets\n(trainDF, testDF) = staticPowerDF.randomSplit([0.80, 0.20], seed=42)\n\n# Set which columns are features\nassembler = VectorAssembler(inputCols=[\"AT\", \"AP\", \"RH\", \"V\"], outputCol=\"features\")\n\n# Reshape the train set\ntrainVecDF = assembler.transform(trainDF)\n\n# Set which column is the label\nlr = LinearRegression(labelCol=\"PE\", featuresCol=\"features\")\n\n# Fit training data\nlrModel = lr.fit(trainVecDF)\n\n# Append predicted PE column, rename it\ntrainPredictionsDF = (lrModel\n  .transform(trainVecDF)\n  .withColumnRenamed(\"prediction\", \"predictedPE\")\n)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Use MLFlow \n\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle. \n\nIn this notebook, we use MLflow to track experiments to record and compare parameters and results.\n\nhttps://www.mlflow.org/docs/latest/index.html"],"metadata":{}},{"cell_type":"markdown","source":["### Post results to MLflow\n\nIn this notebook, we would like to keep track of the Root Mean Squared Error (RMSE).\n\nThis line actually does the work of posting the RMSE to MLflow.\n\n`mlflowClient.logMetric(runId, \"RMSE\", rmse)`\n\nIf you rerun the below cell multiple times, you will see new runs are posted to MLflow, with different RMSE!"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nimport mlflow\nimport os\n\nos.environ[\"MLFLOW_AUTODETECT_EXPERIMENT_ID\"] = 'true'\n\nwith mlflow.start_run(run_name=\"Delta 1\") as run:\n  \n  # Evaluate our result\n  eval = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"predictedPE\", metricName=\"rmse\")\n  rmse = eval.evaluate(trainPredictionsDF)\n\n  mlflow.log_metric(\"RMSE\", rmse)\n  \n  experimentID = run.info.experiment_id\n  \n\ndisplayHTML(\"\"\"<div>RMSE: {}</div>\n               <div>&nbsp;</div>\n               <div>Click \"Runs\" in the upper-right hand corner of the screen to view the results.\"\"\".format(rmse))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Question to Ponder:\n\nWhy is `RMSE` changing under our feet? We are working with \"static\" DataFrames.."],"metadata":{}},{"cell_type":"markdown","source":["You might want to discuss this in class."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n##Introducing Delta Time Travel\n\nDelta time travel allows you to query an older snapshot of a Delta table.\n\nAt the beginning of this lesson, we timestamped our stream i.e.\n\n`.option(\"timestampAsOf\", now)` \n\nWhere `now` is the current timestamp. \n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> It must be a string that can be cast to a timestamp.\n\nLet's wind back to a version of our table we had several hours ago & fit our data to that version.\n\nMaybe some pattern we were looking at became apparent for the first time a few hours ago.\n\nhttps://docs.databricks.com/delta/delta-batch.html#deltatimetravel\n\nThis query shows the timestamps of the Delta writes as they were happening."],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(\"SELECT timestamp FROM (DESCRIBE HISTORY {}) ORDER BY timestamp\".format(powerTable)))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Let's rewind back to almost the beginning (where we had just a handful of rows), let's say the 5th write.\n\nMaybe we started noticing a pattern at this point."],"metadata":{}},{"cell_type":"code","source":["# Pick out 5th write\noldTimestamp = spark.sql(\"SELECT timestamp FROM (DESCRIBE HISTORY {}) ORDER BY timestamp\".format(powerTable)).take(5)[-1].timestamp\n\n# Re-build the DataFrame as it was in the 5th write\nrewoundDF = spark.sql(\"SELECT * FROM {} TIMESTAMP AS OF '{}'\".format(powerTable, oldTimestamp))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["We had this many (few) rows back then."],"metadata":{}},{"cell_type":"code","source":["rewoundDF.count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display(rewoundDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Train Model Based on Data from a Few Hours Ago\n\n* Use `rewoundDF`\n* Write to MLflow\n\nNotice the only change from what we did earlier is the use of `rewoundDF`"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Split DataFrame into test/train sets\n(trainDF, testDF) = staticPowerDF.randomSplit([0.80, 0.20], seed=42)\n\n# Set which columns are features\nassembler = VectorAssembler(inputCols=[\"AT\", \"AP\", \"RH\", \"V\"], outputCol=\"features\")\n\n# Reshape the train set\ntrainVecDF = assembler.transform(trainDF)\n\n# Set which column is the label\nlr = LinearRegression(labelCol=\"PE\", featuresCol=\"features\")\n\n# Fit training data\nlrModel = lr.fit(trainVecDF)\n\n# Append predicted PE column, rename it\ntrainPredictionsDF = (lrModel\n  .transform(trainVecDF)\n  .withColumnRenamed(\"prediction\", \"predictedPE\")\n)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nimport mlflow\n\nwith mlflow.start_run(run_name=\"Delta 2\") as run:\n  \n  # Evaluate our result\n  eval = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"predictedPE\", metricName=\"rmse\")\n  rmse = eval.evaluate(trainPredictionsDF)\n\n  # Log the results with MLFlow\n  mlflow.log_metric(\"RMSE\", rmse)\n  \n# Display some results below\ndisplayHTML(\"\"\"<div>RMSE: {}</div>\n               <div>&nbsp;</div>\n               <div>Click \"Runs\" in the upper-right hand corner of the screen to view the results.\"\"\".format(rmse))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Evaluate Using Test Set\n\n0. Reshape data via `assembler.transform()`\n0. Apply linear regression model \n0. Record metrics in MLflow"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nlrPipeline = Pipeline(stages=[assembler, lr])\n\n# Pipelines are themselves Estimators -- so to use them we call fit:\nlrModel = lrPipeline.fit(trainDF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.evaluation import RegressionMetrics\nimport mlflow\n\nwith mlflow.start_run(run_name=\"Delta 3\") as run:\n  justPredictionAndLabelDF = lrModel.transform(testDF).select(\"prediction\", \"PE\")\n  metrics = RegressionMetrics(justPredictionAndLabelDF.rdd.map(lambda r: (r.prediction, r.PE)))\n  rmse = metrics.rootMeanSquaredError\n\n  # Log the results with MLFlow\n  mlflow.log_metric(\"RMSE\", rmse)\n  \n# Display some results below\ndisplayHTML(\"\"\"<div>RMSE: {}</div>\n               <div>&nbsp;</div>\n               <div>Click \"Runs\" in the upper-right hand corner of the screen to view the results.\"\"\".format(rmse))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["-sandbox\n### Final Model\n\nThe stats from the test set are pretty good so we've done a decent job coming up with the model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression, LinearRegressionModel\nlrPipeline = Pipeline()\n\nweights = lrModel.stages[1].coefficients\nintercept = lrModel.stages[1].intercept"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["print(\"The equation that describes the relationship between AT, AP, RH and PE is:\\nPE = {} - {} * AT + {} * AP - {} * RH - {} * V\"  \n      .format(intercept, abs(weights[0]), weights[1], abs(weights[2]), abs(weights[3])))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["We are pretty happy with the model we developed.\n\nLet's save the model."],"metadata":{}},{"cell_type":"code","source":["fileName = experimentPath + \"/model\"\nlrModel.write().overwrite().save(fileName)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["## Make real-time predictions using the data from the stream.\n\nLet's apply the model we saved to the rest of the streaming data!"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\nlrPredModel = PipelineModel.load(fileName)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Time to make some predictions!!"],"metadata":{}},{"cell_type":"code","source":["stream = (lrPredModel\n          .transform(initialDF)\n          .withColumnRenamed(\"prediction\", \"PredictedPE\"))\n\ndisplay(stream.select(\"AT\", \"AP\", \"V\", \"RH\", \"PE\", \"PredictedPE\"))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## Clean Up"],"metadata":{}},{"cell_type":"markdown","source":["Stop all remaining streams."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n  s.stop()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Drop the table we are using."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS {}\".format(powerTable))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["Delete directories we were writing to."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(writePath, True)\ndbutils.fs.rm(checkpointPath, True)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup-08\"\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"11-Delta-Time-Travel","notebookId":1121904905107145},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Lab: Post-Processing on a Data Stream\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you:<br>\n - Create a data stream and train a random forest model\n - Defining post-processing logic \n - Apply logic to a data stream\n - Write a DataFrame to a scalable Delta format"],"metadata":{}},{"cell_type":"code","source":["%run \"./../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Creating Data Stream and Training Model\n\nImport the same Airbnb dataset."],"metadata":{}},{"cell_type":"code","source":["airbnbDF = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet/\")\n\ndisplay(airbnbDF)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Create a data stream to make predictions off of. Fill in the schema field with the appropriate airbnbDF schema."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# ANSWER\nstreamingData = (spark\n                 .readStream\n                 .schema(airbnbDF.schema)\n                 .option(\"maxFilesPerTrigger\", 1)\n                 .parquet(\"/mnt/training/airbnb/sf-listings/airbnb-cleaned-mlflow.parquet/\")\n                 .drop(\"price\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Run the following cell to train a random forest model `rf` for making price predictions."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\ndf = airbnbDF.toPandas()\nX_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n\n# new random forest model\nrf = RandomForestRegressor(n_estimators=100, max_depth=25)\n\n# fit and evaluate new rf model\nrf.fit(X_train, y_train)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Define Post-Processing Logic"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nWhen processing our data stream, we are interested in seeing, for each data point, whether the predicted price is \"High\", \"Medium\", or \"Low\". To accomplish this, we are going to define a model class which will apply the desired post-processing step to our random forest `rf`'s results with a `.predict()` call.\n\nComplete the `postprocess_result()` function to change the predicted value from a number to one of 3 categorical labels, \"High\", \"Medium\", or \"Low\". Then finish the line in `predict()` to return the desired output.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This can be done in pure Python or, for a more performant solution, using broadcasting on a `pandas` series or DataFrame."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nimport mlflow\nfrom mlflow.pyfunc import PythonModel\n\n# Define the model class\nclass streaming_model(PythonModel):\n\n    def __init__(self, trained_rf):\n        self.rf = trained_rf\n\n    def postprocess_result(self, results):\n        '''return post-processed results\n        High: predicted price >= 120\n        Medium: predicted price < 120 and >= 70\n        Low: predicted price < 70'''\n        output = []\n        for result in results:\n          if result >= 120:\n            output.append(\"High\")\n          elif result >= 70:\n            output.append(\"Medium\")\n          else:\n            output.append(\"Low\")\n        return output\n    \n    def predict(self, context, model_input):\n        results = self.rf.predict(model_input)\n        return self.postprocess_result(results)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Run the following cell to create and save your model at `model_path`."],"metadata":{}},{"cell_type":"code","source":["# Construct and save the model\nmodel_path = userhome + \"/ml-production/07_streaming_model/\"\ndbutils.fs.rm(model_path.replace(\"/dbfs\", \"\"), True) # remove folder if already exists\n\nmodel = streaming_model(trained_rf = rf)\nmlflow.pyfunc.save_model(dst_path=model_path, python_model=model)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["The next cell will test your `streaming_model`'s `.predict()` function on fixed data `X_test` (not a data stream). You should see a list of price labels output underneath the cell."],"metadata":{}},{"cell_type":"code","source":["# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_pyfunc(model_path)\n\n# Apply the model\nloaded_model.predict(X_test)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Apply Post-Processing Step to Data Stream"],"metadata":{}},{"cell_type":"markdown","source":["Finally, after confirming that your model works properly, apply it in parallel on all rows of `streamingData`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nimport mlflow.pyfunc\n\n# Load the model in as a spark UDF\npyfunc_udf = mlflow.pyfunc.spark_udf(spark, model_path, result_type=\"string\")\n\n# Apply UDF to data stream\npredictionsDF = streamingData.withColumn(\"prediction\", pyfunc_udf(*streamingData.columns))\n\ndisplay(predictionsDF.select(\"prediction\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Write DataFrame to Parquet"],"metadata":{}},{"cell_type":"markdown","source":["Now continuously write `predictionsDF` to a parquet file as they get created by the model."],"metadata":{}},{"cell_type":"code","source":["checkpointLocation = userhome + \"/academy/stream.checkpoint\"\nwritePath = userhome + \"/academy/predictions\"\n\n(streamingData\n  .writeStream                                           # Write the stream\n  .format(\"delta\")                                       # Use the delta format\n  .partitionBy(\"zipcode\")                                # Specify a feature to partition on\n  .option(\"checkpointLocation\", checkpointLocation)      # Specify where to log metadata\n  .option(\"path\", writePath)                             # Specify the output path\n  .outputMode(\"append\")                                  # Append new records to the output path\n  .start()                                               # Start the operation\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Check that your predictions are indeed being written out to `writePath`."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.ls(writePath)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Run the following cell to terminate all active streams."],"metadata":{}},{"cell_type":"code","source":["# stop streams\n[q.stop() for q in spark.streams.active]\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"07-Lab","notebookId":1121904905106349},"nbformat":4,"nbformat_minor":0}
